Quick links: [Class Preparation](#class-preparation) | [Day One](#day-one-in-class-materials) | [Day Two](#day-two-in-class-materials) 

## Experiments on the Embodied Web 

New York University  
Tisch School of the Arts  
ITPG-GT 3013 - 001  
Experiments on the Embodied Web  
Fall 2024  
Course Credits — 1  

Saturday, 10/19 11:40am - 6:10pm  
Sunday, 10/20 12:10pm - 6:10pm  

## Instructor Contact

Instructor: Lisa Jamhoury  
Email: lisa.jamhoury@nyu.edu

## Course Objectives
At the completion of this course, the student will be able to:

- Think critically about the role of the physical body in web environments
- Better understand motion capture and how the body is represented digitally 

## Course Description

Today’s internet, made up of mostly text documents and two-dimensional images and videos, is the result of historical limitations in bandwidth, graphics processing and input devices. These limitations have made the internet a place where the mind goes, but the body cannot follow. Recent advances in motion capture devices, graphics processing, machine learning, bandwidth and browsers, however, are paving the way for the body to find its place online. Experiments on the Embodied Web will explore the new realm of embodied interactions in the browser across networks. The course will include discussion of influential works in the development of online embodied interaction, including the works of Kit Galloway and Sherrie Rabinowitz, Susan Kozel, and Laurie Anderson. Together we’ll explore pose detection across webRTC peer connections in p5.js and Three.js. Experience with Node, HTML and JavaScript is helpful but not required. ICM level programming experience is required.

## Evaluation and Policies

See [Evaluation and Policies](/policies.md).

## Mode of Instruction

This course will be offered entirely in person. If you need special accomodations please reach out. 

## Support

If you find yourself struggling, remember that you have many forms of support that you can take advantage of at ITP. Look out for the [office hours and help sessions that the residents offer](https://itp.nyu.edu/residents/contact-the-residents/). 

## Class Preparation 

### Required tools for class 

We will be coding using p5 and three.js. If you'd like to follow along in class please make sure you have a code editor and terminal available. I will be using the following tools in class 

- [Visual Studio Code](https://code.visualstudio.com/)
- [p5.vscode](https://marketplace.visualstudio.com/items?itemName=samplavigne.p5-vscode)
- [Live Server](https://marketplace.visualstudio.com/items?itemName=ritwickdey.LiveServer)
- [iTerm](https://iterm2.com/)
- Please also bring laptop, headphones, some paper, and a pen or pencil to class

### Required reading for class ! Complete before the first class! 

Please read the following readings before coming to class. It is only six pages of reading in total, but it is a bit thick. Give yourself some time to read and digest. We'll be discussing the materials at the top of our first class, and I'll be looking forward to hearing your thoughts. 

- Readings located on [Google Drive here](https://drive.google.com/drive/folders/1f38QY-0t23d-S_U3265s8vtKPOPRodG3?usp=sharing). Requires NYU Login to access. 
- [Phenomenology of perception by Maurice Merleau-Ponty](https://drive.google.com/file/d/1xHP96xLGLJasFpEcwCLtYRBVf29x2N0x/view?usp=drive_link), pages 92-94 (through "what sees and touches")
- [Here/There: Telepresence, Touch, and Art at the Interface by Kris Paulsen](https://drive.google.com/file/d/18o0pNtYZz6qwTM3kNTxX41P0NaaCAPya/view?usp=drive_link), section titled "Telepresent Touch" pages 6-8, starting at bottom of page 6.
- [The Extended Mind: The Power of Thinking Outside the Brain](https://drive.google.com/file/d/1aN4eHnycAdRNbcw_qE5w3I9PfP2-yqf-/view?usp=drive_link) Pages 1-2. 
- Exercise: While reading highlight the lines that stand out to you. For each reading, pick JUST ONE SENTENCE from EACH reading (3 sentences total, one from each reading) Place the 3 sentences in [this Google Doc](https://docs.google.com/document/d/1WFhy2c2t_jvf7b-erPVCgwRNSDOiHz5zzDP4Dd4VOY4/edit?usp=sharing) along with your first and last name. Requires NYU Login to access doc. Must be completed before class begins.

## Day One In Class Materials

### Rough Schedule 

11:40 - 1:35pm — Syllabus, Intros, Theory  
 1:35 - 1:50pm — Break 1 (15 minute)  
 1:50 - 3:45pm — Code   
 3:45 - 4:15pm — Break 2 (30 minute)  
 4:15 - 6:10pm — Practice   


### Theory 

- Discussion Topic: What is a body? Should it be online? Why? How? 
- Discussion of homework readings 
- Independent and group activity, [Group Activity Board](https://docs.google.com/presentation/d/1rfqIA6Xt9TIUAlnKEouHUfaCz-RMQv6XuWOljKEdzjg/edit?usp=sharing)
- Mocap history, forms, body politics, [Google Slides](https://docs.google.com/presentation/d/1qgpGtGiLlRR9DyI5A3oMUjLAOiqBMJOShj-jgAmCoL4/edit?usp=sharing)

### Code

- Computer vision: how does the computer see the body
- Understanding framebuffers and keypoints 

#### Examples 

- [Examples](/examples/day_one/)

#### Resources

- [Cat image](examples/assets/cat.jpg)
- [BodyPose](https://docs.ml5js.org/#/reference/bodypose)
- [Keypoint smoothing](https://javascript.plainenglish.io/simple-smoothing-for-posenet-keypoints-cd1bc57f5872)
- [Keypoint smoothing example](https://editor.p5js.org/lisajamhoury/sketches/oB3r4UNOT)

### Practice 

- Create a rough prototype of a single-person experience that takes into account form and quality of movement. The experience should have a clear goal. Before you begin building discuss the following with your partner: 
1. how do you want the participant to feel / move / etc?
2. how can you acheive this through design and code?

## Day Two In Class Materials 

### Rough Schedule 

12:10 - 1:55pm — Theory  
 1:55 - 2:10pm — Break 1 (15 minute)  
 2:10 - 3:55pm — Code 1  
 3:55 - 4:25pm — Break 2 (30 minute)  
 4:25 - 6:10pm — Code 2 and/or Practice    

### Theory 

- Embodied interaction in the "third space" [Google Slides](https://docs.google.com/presentation/d/1sJ4Vd6ZlwUMPAeQ3eFfkatguL00LWJLsXO679xzlBXY/edit?usp=sharing) 
- Closer Reading [pdf](https://github.com/lisajamhoury/The-Body-Everywhere-and-Here-2021/blob/main/readings/closer_2.pdf) | [audio file](https://github.com/lisajamhoury/Experiments-on-the-Embodied-Web-2024/tree/main/media)
- [Group Activity Board](https://docs.google.com/presentation/d/1rfqIA6Xt9TIUAlnKEouHUfaCz-RMQv6XuWOljKEdzjg/edit?usp=sharing) 

### Code

- Peer to peer
- Kinectron, Kinect over peer

#### Examples 

- [Examples](/examples/day_two/)

#### Additional Examples 

- [Kinectron Feed Test, Azure](/examples/other/01-kinectron-feed-test/) 
- [Kinectron Three.js Ribbons](/examples/other/02-kinectron-3js-ribbons/) 
- [Kinectron Three.js Pointcloud](/examples/other/03-kinectron-3js-pointcloud/) 

#### Resources 

- [ml5 BodyPose](https://docs.ml5js.org/#/reference/bodypose)
- [p5LiveMedia](https://github.com/vanevery/p5LiveMedia?tab=readme-ov-file)
- [HSB to RGB](https://stackoverflow.com/questions/17242144/javascript-convert-hsb-hsv-color-to-rgb-accurately/54024653)
- [Kinectron](https://kinectron.github.io/)
- [Kinectron Server Version 0.3.9](https://github.com/kinectron/kinectron/releases/tag/0.3.9)


### Practice 

Create a rough prototype for two or more people that requires some type of interaction between two bodies. Can you interact without using your hands? Before you begin building consider the following questions:

1. What is the interaction you are trying to create? How do you want the participants to feel / move / etc?
2. How can you achieve this through design and code?


## More resources 

### Helpful Links, Resources, Inspo — Day 1
- [Coding Train 9.12: Local Server, Text Editor, JavaScript Console - p5.js Tutorial](https://www.youtube.com/watch?v=UCHzlUiDD10)
- [Coding Train 11.3: The Pixel Array - p5.js Tutorial](https://www.youtube.com/watch?v=nMUMZ5YRxHI)
- [p5 Local Server](https://github.com/processing/p5.js/wiki/Local-server)
- [npm live-server](https://www.npmjs.com/package/live-server)
- [How to Turn Your Smartphone Into a Webcam](https://www.wired.com/story/use-your-phone-as-webcam/#:~:text=Install%20EpocCam%20Webcam%20Viewer%20from,network%20and%20launch%20the%20apps.)
- [Kyle McDonald JS CV Examples](https://kylemcdonald.github.io/cv-examples/) | Some more CV examples Stick to low and medium level for week 1 assignment
- Yining Shi Examples | [Posenet / Bodypix](https://github.com/yining1023/machine-learning-for-the-web/tree/master/week3-pose) | [Handpose / Facemesh](https://github.com/yining1023/machine-learning-for-the-web/tree/master/face-hand)
- [Lingdong Huang Mediapipe Demos](https://github.com/LingDong-/handpose-facemesh-demos)
- [Pose Animator: SVG Characters with Posenet](https://blog.tensorflow.org/2020/05/pose-animator-open-source-tool-to-bring-svg-characters-to-life.html)
- [clmtrackr](https://github.com/auduno/clmtrackr) | [Live example from Kyle McDonald](https://kylemcdonald.github.io/cv-examples/FaceTracking/)
- [tfjs face landmarks documentation](https://github.com/tensorflow/tfjs-models/tree/master/face-landmarks-detection)
- [ml5js.org](https://ml5js.org/)
- Coding Train ml5.js: Pose Estimation with PoseNet: [7.1](https://thecodingtrain.com/learning/ml5/7.1-posenet.html) | [7.2](https://thecodingtrain.com/learning/ml5/7.2-pose-classifier.html) | [7.3](https://thecodingtrain.com/learning/ml5/7.3-pose-regression.html)
- [Tips to improve your generative artwork](https://tylerxhobbs.com/essays/2018/tips-to-improve-your-generative-artwork)

### Helpful Links, Resources, Inspo — Day 2

#### General Kinect 
- [How to pronounce the name of Microsoft's cloud: Azure](https://www.youtube.com/watch?v=AmP11EgEM4g)
- [Kinect Windows / Azure Comparison](https://docs.microsoft.com/en-us/azure/kinect-dk/windows-comparison)
- [Azure Kinect Hardware Specification](https://docs.microsoft.com/en-us/azure/kinect-dk/hardware-specification)
- [Azure Kinect body joints documentation](https://docs.microsoft.com/en-us/azure/Kinect-dk/body-joints)
- [Understanding Kinect V2 Joints and Coordinate System](https://medium.com/@lisajamhoury/understanding-kinect-v2-joints-and-coordinate-system-4f4b90b9df16)

#### Kinectron
- [Kinectron](https://kinectron.github.io/)
- [Kinectron Server Version 0.3.9](https://github.com/kinectron/kinectron/releases/tag/0.3.9)
- [Kinectron Bootcamp Install by Jake Sherwood](https://jakesherwood.com/blog/body_ewah/kinectron-install)
- [Coding Train: Kinectron](https://www.youtube.com/watch?v=BV6xK3EOznI)
- [More Kinectron Examples](https://kinectron.github.io/docs/example-simple-skeleton.html) | Includes [Skeleton Example](https://kinectron.github.io/docs/example-skeleton-images-windows.html) 
— [Kinectron Github Repo Examples](https://github.com/kinectron/kinectron/tree/master/examples) | See [Feed Test Azure](https://github.com/kinectron/kinectron/tree/master/examples/azure_examples/p5_examples/feedTest) or [Feed Test Windows V2](https://github.com/kinectron/kinectron/tree/master/examples/windows_examples/p5_examples/feedTest) to see how all feeds work

#### Three.js 
- [Three.js Introduction](https://threejs.org/docs/#manual/en/introduction/Creating-a-scene)
- [THREE.Meshline](https://github.com/spite/THREE.MeshLine)

#### More resources
- [Kinect 2 Node Module](https://github.com/wouterverweirder/kinect2)
- [Azure Kinect Node Module](https://github.com/wouterverweirder/kinect-azure)
- [depth2web](https://github.com/js6450/depth2web)
- [Olos template](https://github.com/lisajamhoury/olos-template) 

### Further Reading 
- [Artificial Reality by Myron Krueger, pages 91-99](https://github.com/lisajamhoury/The-Body-Everywhere-and-Here-2021/blob/main/readings/artificial_reality_5.pdf)
- [The End of Average, Todd Rose, pages 1-9](https://github.com/lisajamhoury/The-Body-Everywhere-and-Here-2021/blob/main/readings/end_of_avg_intro.pdf)
- ["Mismatch: How Inclusion Shapes Design" Kat Holmes](https://mitpress.mit.edu/books/mismatch)
- [Video: Design for seven billion; design for one - Kat Holmes](https://www.youtube.com/watch?v=vPH1exUrSh8)



